{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "##### Global tuberculosis report cover: WHO has published a global tuberculosis (TB) report every year since 1997.The report provides a comprehensive and up-to-date assessment of the TB epidemic, and of progress in prevention, diagnosis and treatment of the disease at global, regional and country levels.\n",
        "\n",
        "\n",
        "Our goal is to predict the treatment outcome .\n",
        "\n",
        "References: https://www.who.int/teams/global-tuberculosis-programme/data AND https://www.kaggle.com/datasets/mpwolke/cusersmarildownloadstreatmentcsv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z9T1f-tXahwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis identified key features that significantly contribute to the variance in the target variable, providing valuable insights for further investigations or decision-making.\n",
        "\n",
        "\n",
        "Main:\n",
        "How well can the model predict the treatment success rate for new TB cases using dimensionality reduction and top contributing features?\n"
      ],
      "metadata": {
        "id": "cnKeJ4cundva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicts the treatment success rate for new TB cases with high accuracy and robustness.\n",
        "\n",
        " By applying PCA to reduce dimensionality and focusing on the top contributing features, the model explains, on average, 86.75% of the variance in the target variable across different cross-validation folds, with a consistent performance indicated by a low standard deviation of 0.0225. This demonstrates that the model is both accurate and reliable for predicting the treatment success rate, making it a valuable tool for analyzing and predicting outcomes in TB treatment programs."
      ],
      "metadata": {
        "id": "o06I9LAynlKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  !{sys.executable} -m pip -q -q install pycm\n",
        "except:\n",
        "  pass\n",
        "\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "aJCUmU3zLA0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context\n",
        "Data provided by countries to WHO and estimates of TB burden generated by WHO for the Global Tuberculosis Report"
      ],
      "metadata": {
        "id": "UTsEd1okNwF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('treatment.csv', sep=';')"
      ],
      "metadata": {
        "id": "XLHQ9XKcJe3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimal approach to initiate a data analysis is by addressing missing values first. This step enables us to discern the statistical significance of parameters, which is crucial for our analysis.\n"
      ],
      "metadata": {
        "id": "7ej_hocpK6ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "\n",
        "total_values = df.shape[0]\n",
        "\n",
        "percentage_missing = (missing_values / total_values) * 100\n",
        "\n",
        "missing_data = pd.DataFrame({'Column': df.columns, 'Percentage Missing': percentage_missing})\n",
        "\n",
        "missing_data_sorted = missing_data.sort_values(by='Percentage Missing',ascending=False)\n",
        "\n",
        "print(missing_data_sorted)"
      ],
      "metadata": {
        "id": "pF_lVgJXMUWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with over 90% missing values\n",
        "columns_to_drop = missing_data[missing_data['Percentage Missing'] > 90]['Column']\n",
        "df_cleaned = df.drop(columns_to_drop, axis=1)\n",
        "\n",
        "# Print the remaining columnsdf_cleaned = df_cleaned.dropna(subset=['year'])\n",
        "print(\"Remaining columns after dropping columns with over 90% missing values:\")\n",
        "print(df_cleaned.columns)\n"
      ],
      "metadata": {
        "id": "SoWV_t8gMjMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "K9ZlRIIiMwlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can notice that the std for different variables are quite different. A possible approach is to standardize the numerical variables."
      ],
      "metadata": {
        "id": "GK6OKtWd6bR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.describe()"
      ],
      "metadata": {
        "id": "excL_l946OMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.head()"
      ],
      "metadata": {
        "id": "nCZ_Y37a8ykB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['country',\t'iso2',\t'iso3',\t'iso_numeric',\t'g_whoregion','year']\n",
        "columns_to_drop = [col for col in columns_to_drop if col in df_cleaned.columns]\n",
        "\n",
        "if columns_to_drop:\n",
        "    df_cleaned = df_cleaned.drop(columns=columns_to_drop)\n",
        "\n",
        "object_columns = df_cleaned.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(\"Remaining object columns:\")\n",
        "print(object_columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "IPwYcOhy6vp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix on the standardized numeric columns\n",
        "numeric_df = df_cleaned.select_dtypes(include=[np.number])\n",
        "corr_matrix = numeric_df.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "#plt.figure(figsize=(20, 10))\n",
        "#sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "#plt.show()\n",
        "corr_matrix"
      ],
      "metadata": {
        "id": "Lvgy1auR_hpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import numpy as np\n",
        "# Scale the selected features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df_cleaned)\n",
        "\n",
        "# Check for NaNs\n",
        "if np.isnan(X_scaled).any():\n",
        "    imputer = SimpleImputer(strategy='mean')  # You can choose another strategy\n",
        "    X_scaled = imputer.fit_transform(X_scaled)\n",
        "\n",
        "# Confirm no more NaNs\n",
        "assert not np.isnan(X_scaled).any(), \"NaNs remain in the data after imputing\"\n"
      ],
      "metadata": {
        "id": "iBkNQSoMKyK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PCA\n",
        "pca = PCA(n_components=None)  # None means all components are kept\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame with principal components\n",
        "pca_columns = [f'PC{i+1}' for i in range(X_pca.shape[1])]\n",
        "df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
        "\n",
        "# Print explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance ratio by each principal component:\")\n",
        "print(explained_variance)\n",
        "\n",
        "# Visualize the cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o')\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tg-vnk84LCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: This approach may lack significance as there is no assurance that all the data shares the same units. Hence, to ensure a more meaningful analysis, employing standardscaler and PCA (Principal Component Analysis) is essential and stan."
      ],
      "metadata": {
        "id": "-SJkhydlIQ2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the original information. It achieves this by identifying the directions, called principal components, along which the data varies the most.\n",
        "\n",
        "\n",
        "Note: While we have already established the reduced dimensions through an analysis of the percentage of NaN values, in this particular step, our focus lies solely on applying the method rather than revisiting those previous results.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wWGTr69gHxr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_cumu = np.cumsum(pca.explained_variance_ratio_)\n",
        "fig = plt.figure(figsize=[12,8],dpi=200)\n",
        "plt.vlines(x=4, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
        "plt.hlines(y=0.95, xmax=30, xmin=0, colors=\"g\", linestyles=\"--\")\n",
        "plt.plot(var_cumu)\n",
        "plt.ylabel(\"Cumulative variance explained\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B4e8kY3iJMoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pca.head()"
      ],
      "metadata": {
        "id": "Pt09PtFYNZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graphs, we identify the point where the cumulative explained variance reaches approximately 95%. This is the elbow point where adding more components doesn't significantly increase the explained variance.\n",
        "\n",
        "In this case, the elbow point is around 20 components."
      ],
      "metadata": {
        "id": "XP85Bq57MSn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the number of components that capture around 95% of the variance\n",
        "n_components = np.argmax(explained_variance.cumsum() >= 0.95) + 1\n",
        "print(f\"Number of components to retain: {n_components}\")\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_selected = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame with principal components\n",
        "pca_columns = [f'PC{i+1}' for i in range(X_pca_selected.shape[1])]\n",
        "df_pca_selected = pd.DataFrame(X_pca_selected, columns=pca_columns)"
      ],
      "metadata": {
        "id": "tfYEK1IWJ1_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "loading_matrix = pd.DataFrame(loadings, columns=pca_columns)\n",
        "\n",
        "print(\"PCA Loadings (how each original feature contributes to the principal components):\")\n",
        "print(loading_matrix)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "loading_matrix.abs().plot(kind='bar', figsize=(15, 7))\n",
        "plt.title('PCA Loadings (Absolute Values) - Feature Contributions to Principal Components')\n",
        "plt.xlabel('Original Features')\n",
        "plt.ylabel('Loading Value')\n",
        "plt.legend(title='Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tVxPHg1SakfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 5\n",
        "top_features = pd.DataFrame()\n",
        "\n",
        "for i in range(n_components):\n",
        "    pc = f'PC{i+1}'\n",
        "    top_features[pc] = loading_matrix[pc].abs().nlargest(top_n).index\n",
        "\n",
        "print(\"Top contributing features for each principal component:\")\n",
        "print(top_features)\n",
        "\n",
        "# Visualize the top contributing features\n",
        "top_features_plot = loading_matrix.loc[top_features.values.flatten()].abs().plot(kind='bar', figsize=(15, 7))\n",
        "plt.title('Top Contributing Features to Principal Components')\n",
        "plt.xlabel('Original Features')\n",
        "plt.ylabel('Loading Value')\n",
        "plt.legend(title='Principal Components')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yvYTcwIDbjVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Refinement Using Top Contributing Features\n"
      ],
      "metadata": {
        "id": "11-6ueKEct6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imputer = SimpleImputer(strategy='mean')\n",
        "df_cleaned_imputed = pd.DataFrame(imputer.fit_transform(df_cleaned), columns=df_cleaned.columns)\n",
        "\n",
        "target_variable = 'c_new_tsr'\n",
        "\n",
        "if target_variable not in df_cleaned.columns:\n",
        "    raise ValueError(f\"Target variable '{target_variable}' not found in the DataFrame.\")\n",
        "\n",
        "features = df_cleaned_imputed.drop(columns=[target_variable])\n",
        "target = df_cleaned_imputed[target_variable]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(features)\n"
      ],
      "metadata": {
        "id": "RM5sA8oRfM_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Select the number of components that capture around 95% of the variance\n",
        "n_components = np.argmax(explained_variance.cumsum() >= 0.95) + 1\n",
        "print(f\"Number of components to retain: {n_components}\")\n",
        "\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca_selected = pca.fit_transform(X_scaled)\n",
        "\n",
        "pca_columns = [f'PC{i+1}' for i in range(X_pca_selected.shape[1])]\n",
        "df_pca_selected = pd.DataFrame(X_pca_selected, columns=pca_columns)\n",
        "\n",
        "#PCA-transformed DataFrame for verification\n",
        "print(\"DataFrame with PCA-transformed components:\")\n",
        "print(df_pca_selected.head())\n",
        "\n",
        "# Inspect PCA Loadings\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "loading_matrix = pd.DataFrame(loadings, columns=pca_columns, index=features.columns)\n",
        "\n",
        "print(\"PCA Loadings (how each original feature contributes to the principal components):\")\n",
        "print(loading_matrix)\n",
        "\n"
      ],
      "metadata": {
        "id": "RE8vuyKEguJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have tested our model for N=19 features and it does not increase the accuracy of the model. For this reason, we choose 5 features"
      ],
      "metadata": {
        "id": "r071jWYMhtR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top contributing features\n",
        "top_n = 5\n",
        "top_features = pd.DataFrame()\n",
        "\n",
        "for i in range(n_components):\n",
        "    pc = f'PC{i+1}'\n",
        "    top_features[pc] = loading_matrix[pc].abs().nlargest(top_n).index\n",
        "\n",
        "print(\"Top contributing features for each principal component:\")\n",
        "print(top_features)\n",
        "\n",
        "# Flatten the list of top features and remove duplicates\n",
        "top_features_flat = top_features.values.flatten()\n",
        "top_features_set = list(set(top_features_flat))\n",
        "\n",
        "# Create a DataFrame with the features\n",
        "top_features_df = features[top_features_set]\n"
      ],
      "metadata": {
        "id": "5TyqORJNg4lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(top_features_df, target, test_size=0.2, random_state=42)\n",
        "\n",
        "#Train a model (RandomForestRegressor )\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Mean Squared Error with top contributing features: {mse}\")\n",
        "print(f\"Model R^2 Score with top contributing features: {r2}\")"
      ],
      "metadata": {
        "id": "6_u90uVRhAa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Cross-validated R^2 scores: {cv_scores}\")\n",
        "print(f\"Mean cross-validated R^2 score: {np.mean(cv_scores)}\")\n",
        "print(f\"Standard deviation of cross-validated R^2 score: {np.std(cv_scores)}\")\n"
      ],
      "metadata": {
        "id": "526qSkF5h-rU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances = model.feature_importances_\n",
        "feature_names = top_features_set\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances in RandomForestRegressor')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lZfhjzuNiDU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We applied Principal Component Analysis (PCA) to reduce the dimensionality of the dataset. Retained 19 principal components that capture around 95% of the variance in the data.\n"
      ],
      "metadata": {
        "id": "YfmPeQl2m0kf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if our model if we select features with compatible std?"
      ],
      "metadata": {
        "id": "jGEowAcztMyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian inference model using machine learning for predicting the treatment success rate from our dataset, I'll follow several steps. These steps involve preparing our data, selecting a model, and implementing the Bayesian approach. This model allows you to incorporate prior beliefs about the parameters and update these beliefs based on the data."
      ],
      "metadata": {
        "id": "NedfKi2XFCqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned_nonan = df_cleaned.dropna(subset=['c_new_tsr'])\n",
        "\n",
        "X = df_cleaned_nonan[['rep_meth', 'new_sp_coh', 'new_sp_cur', 'new_sp_cmplt', 'new_sp_died', 'new_sp_fail', 'new_sp_def', 'c_new_sp_tsr', 'c_ret_tsr']].values\n",
        "y = df_cleaned_nonan['c_new_tsr'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_train_imputed = imputer.fit_transform(X_train)\n",
        "X_test_imputed = imputer.transform(X_test)\n",
        "\n",
        "print(\"NaN values in X_train_imputed:\", np.isnan(X_train_imputed).sum())\n",
        "print(\"Inf values in X_train_imputed:\", np.isinf(X_train_imputed).sum())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
        "X_test_scaled = scaler.transform(X_test_imputed)\n",
        "\n",
        "# Check for NaN or Inf values after standardization\n",
        "print(\"NaN values in X_train_scaled:\", np.isnan(X_train_scaled).sum())\n",
        "print(\"Inf values in X_train\", np.isnan(X_train_scaled).sum())\n"
      ],
      "metadata": {
        "id": "uYGQZbvlvDuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with pm.Model() as model:\n",
        "    x_shared = pm.MutableData('x', X_train_scaled)\n",
        "    y_shared = pm.MutableData('y', y_train)\n",
        "\n",
        "    # Priors for unknown model parameters\n",
        "    alpha = pm.Normal('alpha', mu=0, sigma=10)\n",
        "    beta = pm.Normal('beta', mu=0, sigma=10, shape=X_train_scaled.shape[1])\n",
        "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
        "\n",
        "    # Expected value of outcome\n",
        "    mu = alpha + pm.math.dot(x_shared, beta)\n",
        "\n",
        "    # Likelihood (sampling distribution) of observations\n",
        "    obs = pm.Normal('obs', mu=mu, sigma=sigma, observed=y_shared)\n",
        "\n",
        "    # Sample from the posterior using Markov chain Monte Carlo\n",
        "    trace = pm.sample(1000, tune=2000, return_inferencedata=True, init='adapt_diag')"
      ],
      "metadata": {
        "id": "l9pEPrBHFOe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(trace)\n",
        "\n",
        "# Summary statistics for the posterior distributions\n",
        "summary = az.summary(trace)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "ATAD3BRYGz3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with model:\n",
        "    pm.set_data({'x': X_test_scaled, 'y': y_test})\n",
        "    post_pred = pm.sample_posterior_predictive(trace, var_names=[\"obs\"], random_seed=42)\n",
        "\n",
        "y_obs_samples = post_pred.posterior_predictive['obs']\n",
        "\n",
        "mean_prediction = y_obs_samples.mean(axis=(0, 1))\n",
        "lower_bound = np.percentile(y_obs_samples, 2.5, axis=(0, 1))\n",
        "upper_bound = np.percentile(y_obs_samples, 97.5, axis=(0, 1))\n",
        "\n",
        "mse_test = mean_squared_error(y_test, mean_prediction)\n",
        "r2_test = r2_score(y_test, mean_prediction)\n",
        "print(f\"Test set Mean Squared Error: {mse_test}\")\n",
        "print(f\"Test set R^2 Score: {r2_test}\")\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(range(len(y_test)), y_test, label='Actual')\n",
        "plt.scatter(range(len(mean_prediction)), mean_prediction, color='red', label='Predicted Mean')\n",
        "plt.fill_between(range(len(y_test)), lower_bound, upper_bound, color='purple', alpha=0.2, label='95% Credible Interval')\n",
        "plt.legend()\n",
        "plt.xlabel('Observation Index')\n",
        "plt.ylabel('Treatment Success Rate')\n",
        "plt.title('Actual vs. Predicted Treatment Success Rates on Test Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NRIoPg2kMZaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}